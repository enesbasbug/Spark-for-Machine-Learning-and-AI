{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!java -version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- wget https://dlcdn.apache.org/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz\n",
    "- tar xf spark-*\n",
    "- sudo mv spark-3.2.0-bin-hadoop3.2 /opt/spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install findspark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## There are two solutions\n",
    "\n",
    " copy downloaded spark folder in somewhere in C directory and give the link as below\n",
    "\n",
    "- import findspark\n",
    "- findspark.init('C:/spark')\n",
    "- use the function of findspark to find automatically the spark folder\n",
    "#### \n",
    "\n",
    "- import findspark\n",
    "- findspark.find()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/38411914/the-spark-home-env-variable-is-set-but-jupyter-notebook-doesnt-see-it-windows?rq=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.14180084\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init('spark-3.2.0-bin-hadoop3.2')\n",
    "\n",
    "import pyspark\n",
    "import random\n",
    "\n",
    "\"\"\" Testing\n",
    "If I have spark and it works properly\n",
    "\"\"\"\n",
    "sc = pyspark.SparkContext(appName=\"Pi\")\n",
    "num_samples = 100000000\n",
    "\n",
    "def inside(p):     \n",
    "  x, y = random.random(), random.random()\n",
    "  return x*x + y*y < 1\n",
    "\n",
    "count = sc.parallelize(range(0, num_samples)).filter(inside).count()\n",
    "\n",
    "pi = 4 * count / num_samples\n",
    "print(pi)\n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "sc = SparkContext('local')\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "# https://stackoverflow.com/questions/39541204/pyspark-nameerror-name-spark-is-not-defined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation and Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Numeric\n",
    "    - MinMaxScaler\n",
    "    - StandardScaler\n",
    "    - Bucketizer\n",
    "- ### Text\n",
    "    - Tokenizer\n",
    "    - HashingTF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize Numeric Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features_df = spark.createDataFrame([\n",
    "    (1, Vectors.dense([10.0, 10000.0, 1.0]),),\n",
    "    (2, Vectors.dense([20.0, 30000.0, 2.0]),),\n",
    "    (3, Vectors.dense([30.0, 40000.0, 3.0]))\n",
    "], [\"id\",\"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1, features=DenseVector([10.0, 10000.0, 1.0]))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_scaler = MinMaxScaler(inputCol = \"features\", outputCol = \"sfeatures\")\n",
    "smodel = feature_scaler.fit(features_df)\n",
    "sfeatures_df = smodel.transform(features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1, features=DenseVector([10.0, 10000.0, 1.0]), sfeatures=SparseVector(3, {}))]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sfeatures_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1, features=DenseVector([10.0, 10000.0, 1.0]), sfeatures=SparseVector(3, {})),\n",
       " Row(id=2, features=DenseVector([20.0, 30000.0, 2.0]), sfeatures=DenseVector([0.5, 0.6667, 0.5])),\n",
       " Row(id=3, features=DenseVector([30.0, 40000.0, 3.0]), sfeatures=DenseVector([1.0, 1.0, 1.0]))]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sfeatures_df.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+\n",
      "|          features|           sfeatures|\n",
      "+------------------+--------------------+\n",
      "|[10.0,10000.0,1.0]|           (3,[],[])|\n",
      "|[20.0,30000.0,2.0]|[0.5,0.6666666666...|\n",
      "|[30.0,40000.0,3.0]|       [1.0,1.0,1.0]|\n",
      "+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sfeatures_df.select(\"features\", \"sfeatures\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize numeric data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = spark.createDataFrame([\n",
    "    (1, Vectors.dense([10.0, 10000.0, 1.0]),),\n",
    "    (2, Vectors.dense([20.0, 30000.0, 2.0]),),\n",
    "    (3, Vectors.dense([30.0, 40000.0, 3.0]))\n",
    "], [\"id\",\"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1, features=DenseVector([10.0, 10000.0, 1.0]))]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_stand_scaler = StandardScaler(inputCol = \"features\", outputCol = \"sfeatures\", withStd = True, withMean = True)\n",
    "stand_smodel = feature_stand_scaler.fit(features_df)\n",
    "stand_sfeatures_df = stand_smodel.transform(features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1, features=DenseVector([10.0, 10000.0, 1.0]), sfeatures=DenseVector([-1.0, -1.0911, -1.0]))]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stand_sfeatures_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1, features=DenseVector([10.0, 10000.0, 1.0]), sfeatures=DenseVector([-1.0, -1.0911, -1.0])),\n",
       " Row(id=2, features=DenseVector([20.0, 30000.0, 2.0]), sfeatures=DenseVector([0.0, 0.2182, 0.0])),\n",
       " Row(id=3, features=DenseVector([30.0, 40000.0, 3.0]), sfeatures=DenseVector([1.0, 0.8729, 1.0]))]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stand_sfeatures_df.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+--------------------+\n",
      "| id|          features|           sfeatures|\n",
      "+---+------------------+--------------------+\n",
      "|  1|[10.0,10000.0,1.0]|[-1.0,-1.09108945...|\n",
      "|  2|[20.0,30000.0,2.0]|[0.0,0.2182178902...|\n",
      "|  3|[30.0,40000.0,3.0]|[1.0,0.8728715609...|\n",
      "+---+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stand_sfeatures_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bucketize Numeric Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Bucketizer # allows me to group data based on boundaries\n",
    "splits = [-float(\"inf\"), -10.0, 0.0, 10.0, float(\"inf\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|features|\n",
      "+--------+\n",
      "|  -800.0|\n",
      "|   -10.5|\n",
      "|    -1.7|\n",
      "|     0.0|\n",
      "|     8.2|\n",
      "|    90.1|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "b_data = [(-800.0,), (-10.5,), (-1.7,), (0.0,), (8.2,), (90.1,)]\n",
    "b_df = spark.createDataFrame(b_data, [\"features\"])\n",
    "b_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|features|bfeatures|\n",
      "+--------+---------+\n",
      "|  -800.0|      0.0|\n",
      "|   -10.5|      0.0|\n",
      "|    -1.7|      1.0|\n",
      "|     0.0|      2.0|\n",
      "|     8.2|      2.0|\n",
      "|    90.1|      3.0|\n",
      "+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bucketizer = Bucketizer(splits=splits, inputCol=\"features\", outputCol=\"bfeatures\")\n",
    "bucketed_df = bucketizer.transform(b_df)\n",
    "bucketed_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_df = spark.createDataFrame([(1, \"This is an introduction to Spark MLlib\"),\n",
    "                                     (2, \"MLlib includes libraries for classification and regression\"),\n",
    "                                     (3, \"It also contains supporting tools for pipelines\")],\n",
    "                                     [\"id\", \"sentence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|            sentence|\n",
      "+---+--------------------+\n",
      "|  1|This is an introd...|\n",
      "|  2|MLlib includes li...|\n",
      "|  3|It also contains ...|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentences_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+\n",
      "| id|            sentence|               words|\n",
      "+---+--------------------+--------------------+\n",
      "|  1|This is an introd...|[this, is, an, in...|\n",
      "|  2|MLlib includes li...|[mllib, includes,...|\n",
      "|  3|It also contains ...|[it, also, contai...|\n",
      "+---+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sent_token = Tokenizer(inputCol = \"sentence\", outputCol = \"words\")\n",
    "sent_tokenized_df = sent_token.transform(sentences_df)\n",
    "sent_tokenized_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF - IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, sentence: string]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1, sentence='This is an introduction to Spark MLlib')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1, sentence='This is an introduction to Spark MLlib', words=['this', 'is', 'an', 'introduction', 'to', 'spark', 'mllib'])]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenized_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1, sentence='This is an introduction to Spark MLlib', words=['this', 'is', 'an', 'introduction', 'to', 'spark', 'mllib'], rawFeatures=SparseVector(20, {6: 2.0, 8: 1.0, 9: 1.0, 10: 1.0, 13: 1.0, 15: 1.0}))]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashingTF = HashingTF(inputCol = \"words\", outputCol = \"rawFeatures\", numFeatures=20)\n",
    "sent_hfTf_df = hashingTF.transform(sent_tokenized_df)\n",
    "sent_hfTf_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1, sentence='This is an introduction to Spark MLlib', words=['this', 'is', 'an', 'introduction', 'to', 'spark', 'mllib'], rawFeatures=SparseVector(20, {6: 2.0, 8: 1.0, 9: 1.0, 10: 1.0, 13: 1.0, 15: 1.0}), idf_Features=SparseVector(20, {6: 0.5754, 8: 0.2877, 9: 0.6931, 10: 0.6931, 13: 0.6931, 15: 0.2877}))]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf = IDF(inputCol = \"rawFeatures\", outputCol = \"idf_Features\")\n",
    "idfModel = idf.fit(sent_hfTf_df)\n",
    "tfidf_df = idfModel.transform(sent_hfTf_df)\n",
    "tfidf_df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### K-means = useful for data exploration of small and mid-sized data sets \n",
    "- ### Hierarchical clustering - useful for clustering large data sets\n",
    "- ### Different algorithms may find different cluster centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[col1: int, col2: int, col3: int]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_df = spark.read.csv(\"Exercise\\ Files/Ch03/03_02\", header=True, inferSchema=True)\n",
    "cluster_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|col1|col2|col3|\n",
      "+----+----+----+\n",
      "|   7|   4|   1|\n",
      "|   7|   7|   9|\n",
      "|   7|   9|   6|\n",
      "|   1|   6|   5|\n",
      "|   6|   7|   7|\n",
      "|   7|   9|   4|\n",
      "|   7|  10|   6|\n",
      "|   7|   8|   2|\n",
      "|   8|   3|   8|\n",
      "|   4|  10|   5|\n",
      "|   7|   4|   5|\n",
      "|   7|   8|   4|\n",
      "|   2|   5|   1|\n",
      "|   2|   6|   2|\n",
      "|   2|   3|   8|\n",
      "|   3|   9|   1|\n",
      "|   4|   2|   9|\n",
      "|   1|   7|   1|\n",
      "|   6|   2|   3|\n",
      "|   4|   1|   9|\n",
      "+----+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cluster_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster_df.show(75) # There are 3 clusters in these you may observe them comparing the numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+--------------+\n",
      "|col1|col2|col3|      features|\n",
      "+----+----+----+--------------+\n",
      "|   7|   4|   1| [7.0,4.0,1.0]|\n",
      "|   7|   7|   9| [7.0,7.0,9.0]|\n",
      "|   7|   9|   6| [7.0,9.0,6.0]|\n",
      "|   1|   6|   5| [1.0,6.0,5.0]|\n",
      "|   6|   7|   7| [6.0,7.0,7.0]|\n",
      "|   7|   9|   4| [7.0,9.0,4.0]|\n",
      "|   7|  10|   6|[7.0,10.0,6.0]|\n",
      "|   7|   8|   2| [7.0,8.0,2.0]|\n",
      "|   8|   3|   8| [8.0,3.0,8.0]|\n",
      "|   4|  10|   5|[4.0,10.0,5.0]|\n",
      "|   7|   4|   5| [7.0,4.0,5.0]|\n",
      "|   7|   8|   4| [7.0,8.0,4.0]|\n",
      "|   2|   5|   1| [2.0,5.0,1.0]|\n",
      "|   2|   6|   2| [2.0,6.0,2.0]|\n",
      "|   2|   3|   8| [2.0,3.0,8.0]|\n",
      "|   3|   9|   1| [3.0,9.0,1.0]|\n",
      "|   4|   2|   9| [4.0,2.0,9.0]|\n",
      "|   1|   7|   1| [1.0,7.0,1.0]|\n",
      "|   6|   2|   3| [6.0,2.0,3.0]|\n",
      "|   4|   1|   9| [4.0,1.0,9.0]|\n",
      "+----+----+----+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorAssembler = VectorAssembler(inputCols = [\"col1\", \"col2\", \"col3\"], outputCol = \"features\")\n",
    "vcluster_df = vectorAssembler.transform(cluster_df)\n",
    "vcluster_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans().setK(3)\n",
    "kmeans = kmeans.setSeed(1)\n",
    "kmodel = kmeans.fit(vcluster_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([35.88461538, 31.46153846, 34.42307692]),\n",
       " array([80.        , 79.20833333, 78.29166667]),\n",
       " array([5.12, 5.84, 4.84])]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centers = kmodel.clusterCenters()\n",
    "centers # it shows mid , last and first clusters. You may observe them below by their range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vcluster_df.show() # to remember again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import BisectingKMeans\n",
    "bkmeans = BisectingKMeans().setK(3)\n",
    "bkmeans = bkmeans.setSeed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([5.12, 5.84, 4.84]),\n",
       " array([35.88461538, 31.46153846, 34.42307692]),\n",
       " array([80.        , 79.20833333, 78.29166667])]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bkmodel = bkmeans.fit(vcluster_df)\n",
    "bkcenters = bkmodel.clusterCenters()\n",
    "bkcenters #it appeared the same values with 'centers'; however, it doesn't has to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Naive Bayes\n",
    "- ### Multilayer Perceptron\n",
    "- ### Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c0=5.1, _c1=3.5, _c2=1.4, _c3=0.2, _c4='Iris-setosa')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_df = spark.read.csv(\"Exercise\\ Files/Ch04/iris.data\", inferSchema=True)\n",
    "iris_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(sepal_length=5.1, sepal_width=3.5, petal_length=1.4, petal_width=0.2, species='Iris-setosa')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_df = iris_df.select(col(\"_c0\").alias(\"sepal_length\"),\n",
    "                        col(\"_c1\").alias(\"sepal_width\"),\n",
    "                        col(\"_c2\").alias(\"petal_length\"),\n",
    "                        col(\"_c3\").alias(\"petal_width\"),\n",
    "                        col(\"_c4\").alias(\"species\"))\n",
    "iris_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(sepal_length=5.1, sepal_width=3.5, petal_length=1.4, petal_width=0.2, species='Iris-setosa', features=DenseVector([5.1, 3.5, 1.4, 0.2]))]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorAssembler = VectorAssembler(inputCols=[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"], outputCol = \"features\")\n",
    "viris_df = vectorAssembler.transform(iris_df)\n",
    "viris_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+-----------+-----------------+-----+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|    species|         features|label|\n",
      "+------------+-----------+------------+-----------+-----------+-----------------+-----+\n",
      "|         5.1|        3.5|         1.4|        0.2|Iris-setosa|[5.1,3.5,1.4,0.2]|  0.0|\n",
      "+------------+-----------+------------+-----------+-----------+-----------------+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indexer = StringIndexer(inputCol = \"species\", outputCol =\"label\")\n",
    "iviris_df = indexer.fit(viris_df).transform(viris_df)\n",
    "iviris_df.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = iviris_df.randomSplit([0.6, 0.4], 1)\n",
    "train_df = splits[0]\n",
    "test_df = splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98\n",
      "52\n",
      "150\n"
     ]
    }
   ],
   "source": [
    "print(train_df.count())\n",
    "print(test_df.count())\n",
    "print(iviris_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(sepal_length=4.3, sepal_width=3.0, petal_length=1.1, petal_width=0.1, species='Iris-setosa', features=DenseVector([4.3, 3.0, 1.1, 0.1]), label=0.0, rawPrediction=DenseVector([-9.9894, -11.3476, -11.902]), probability=DenseVector([0.7118, 0.183, 0.1051]), prediction=0.0)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = NaiveBayes(modelType=\"multinomial\")\n",
    "nbmodel = nb.fit(train_df)\n",
    "predictions_df = nbmodel.transform(test_df)\n",
    "predictions_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9807692307692307"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol = \"label\", predictionCol = \"prediction\", metricName=\"accuracy\")\n",
    "nbaccuracy = evaluator.evaluate(predictions_df)\n",
    "nbaccuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6923076923076923"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "layers = [4, 5, 5, 3]\n",
    "mlp = MultilayerPerceptronClassifier(layers= layers, seed= 1)\n",
    "mlp_model = mlp.fit(train_df)\n",
    "mlp_predictions = mlp_model.transform(test_df)\n",
    "mlp_evaluator = MulticlassClassificationEvaluator(metricName= \"accuracy\")\n",
    "mlp_accuracy = mlp_evaluator.evaluate(mlp_predictions)\n",
    "mlp_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees Classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9423076923076923"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(labelCol = \"label\", featuresCol = \"features\")\n",
    "dt_model = dt.fit(train_df)\n",
    "dt_predictions = dt_model.transform(test_df)\n",
    "dt_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "dt_accuracy = dt_evaluator.evaluate(dt_predictions)\n",
    "dt_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Linear Regression\n",
    "- ### Decision Tree Regression\n",
    "- ### Gradient-boosted Tree Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About Data #https://archive.ics.uci.edu/ml/machine-learning-databases/00294/\n",
    "- Column AT = Ambient Temperature\n",
    "- V = Vacuum\n",
    "- AP = Ambient Pressure\n",
    "- RH = Relative Humidity\n",
    "- PE = Measure of how much power was generated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[AT: double, V: double, AP: double, RH: double, PE: double]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "pp_df = spark.read.csv(\"Exercise\\ Files/power_plant.csv\", header=True, inferSchema=True)\n",
    "pp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(AT=14.96, V=41.76, AP=1024.07, RH=73.17, PE=463.26, features=DenseVector([14.96, 41.76, 1024.07, 73.17]))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "vectorAssembler = VectorAssembler(inputCols=[\"AT\",\"V\",\"AP\",\"RH\"], outputCol=\"features\")\n",
    "vpp_df = vectorAssembler.transform(pp_df)\n",
    "vpp_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([-1.9775, -0.2339, 0.0621, -0.1581])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LinearRegression(featuresCol = \"features\", labelCol = \"PE\")\n",
    "lr_model = lr.fit(vpp_df)\n",
    "lr_model.coefficients # list which correspond to the coefficients \n",
    "                    # of the different variables that we were \n",
    "                #using to build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "454.6092744523414"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model.intercept # that gives us a point where the line crosses the Y axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So basically, what we've done is we've fit a line to our data \n",
    "# and that's what linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.557126016749488"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model.summary.rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model.save(\"lr1.model\") # saved model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6701\n",
      "2867\n",
      "9568\n"
     ]
    }
   ],
   "source": [
    "# splitting our data into train and test sets\n",
    "splits = vpp_df.randomSplit([0.7,0.3]) #70% and 30%\n",
    "train_df = splits[0]\n",
    "test_df = splits[1]\n",
    "print(train_df.count())\n",
    "print(test_df.count())\n",
    "print(vpp_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.509122685272387"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = DecisionTreeRegressor(featuresCol=\"features\", labelCol=\"PE\")\n",
    "dt_model = dt.fit(train_df)\n",
    "dt_predictions = dt_model.transform(test_df)\n",
    "dt_evaluator = RegressionEvaluator(labelCol=\"PE\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = dt_evaluator.evaluate(dt_predictions)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient-boosted Tree Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.088762719816632"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "gbt = GBTRegressor(featuresCol=\"features\", labelCol = \"PE\")\n",
    "gbt_model = gbt.fit(train_df)\n",
    "gbt_predictions = gbt_model.transform(test_df)\n",
    "gbt_evaluator = RegressionEvaluator(labelCol=\"PE\", predictionCol=\"prediction\", metricName=\"rmse\" )\n",
    "gbt_rmse = gbt_evaluator.evaluate(gbt_predictions)\n",
    "gbt_rmse # it's a little better than both the decision tree and the linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
